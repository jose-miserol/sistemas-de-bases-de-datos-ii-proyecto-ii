{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üìä Notebook 04: Validaci√≥n y M√©tricas de Rendimiento\n",
        "\n",
        "Este notebook recopila **autom√°ticamente** las m√©tricas de los notebooks anteriores y ejecuta las consultas anal√≠ticas.\n",
        "\n",
        "**Prerrequisitos:**\n",
        "- Ejecutar `02_generador_datos.ipynb` (guarda m√©tricas de ingesta)\n",
        "- Ejecutar `03_etl_spark.ipynb` (guarda m√©tricas de ETL)\n",
        "- Los notebooks anteriores guardan sus tiempos en `metricas.json` (mismo directorio)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Configuraci√≥n e Importaciones"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import time\n",
        "import json\n",
        "import os\n",
        "from datetime import datetime\n",
        "\n",
        "# Archivo de m√©tricas compartido (en el mismo directorio de notebooks)\n",
        "METRICS_FILE = 'metricas.json'\n",
        "\n",
        "print(f\"üìÖ Fecha de ejecuci√≥n: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Cargar M√©tricas de Notebooks Anteriores (AUTOM√ÅTICO)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =====================================================\n",
        "# üì• CARGAR M√âTRICAS AUTOM√ÅTICAMENTE\n",
        "# =====================================================\n",
        "\n",
        "if os.path.exists(METRICS_FILE):\n",
        "    with open(METRICS_FILE, 'r') as f:\n",
        "        metricas = json.load(f)\n",
        "    print(\"‚úÖ M√©tricas cargadas autom√°ticamente desde notebooks anteriores\")\n",
        "    print(f\"\\nüìã Contenido de {METRICS_FILE}:\")\n",
        "    print(json.dumps(metricas, indent=2))\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è No se encontr√≥ el archivo de m√©tricas.\")\n",
        "    print(\"   Ejecuta primero los notebooks 02 y 03.\")\n",
        "    metricas = {}\n",
        "\n",
        "# Extraer valores para usar en el resumen\n",
        "tiempo_ingesta = metricas.get('ingesta_cassandra', {}).get('tiempo_segundos', 0)\n",
        "tiempo_etl = metricas.get('etl_spark', {}).get('tiempo_total', 0)\n",
        "\n",
        "print(f\"\\nüïê Tiempos extra√≠dos:\")\n",
        "print(f\"   - Ingesta Cassandra: {tiempo_ingesta:.2f} seg\")\n",
        "print(f\"   - ETL Spark Total:   {tiempo_etl:.2f} seg\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Conexi√≥n a ClickHouse"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Instalar cliente si no existe\n",
        "!pip install -q clickhouse-connect\n",
        "\n",
        "import clickhouse_connect\n",
        "\n",
        "# Conectar a ClickHouse\n",
        "client = clickhouse_connect.get_client(\n",
        "    host='clickhouse',\n",
        "    port=8123,\n",
        "    database='dw_analitico'\n",
        ")\n",
        "\n",
        "# Verificar conexi√≥n\n",
        "result = client.query(\"SELECT COUNT(*) as total FROM ventas_resumen\")\n",
        "total_registros = result.result_rows[0][0]\n",
        "print(f\"‚úÖ Conectado a ClickHouse\")\n",
        "print(f\"   - Registros en ventas_resumen: {total_registros:,}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Consulta 1: Top 10 Categor√≠as por Volumen de Ventas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "query_top10 = \"\"\"\n",
        "SELECT \n",
        "    categoria,\n",
        "    sum(ventas_totales) as total_ventas_periodo\n",
        "FROM dw_analitico.ventas_resumen\n",
        "GROUP BY categoria\n",
        "ORDER BY total_ventas_periodo DESC\n",
        "LIMIT 10\n",
        "\"\"\"\n",
        "\n",
        "print(\"--- Ejecutando Consulta Top 10 ---\")\n",
        "start_time = time.time()\n",
        "\n",
        "result_top10 = client.query(query_top10)\n",
        "\n",
        "end_time = time.time()\n",
        "tiempo_consulta_top10 = end_time - start_time\n",
        "\n",
        "print(f\"‚è±Ô∏è Tiempo de ejecuci√≥n: {tiempo_consulta_top10:.4f} segundos\")\n",
        "print(\"\\nüìä Resultados:\")\n",
        "print(\"-\" * 40)\n",
        "for row in result_top10.result_rows:\n",
        "    print(f\"  {row[0]}: ${row[1]:,.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Consulta 2: Promedio de Ventas Diarias por Categor√≠a"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "query_promedio = \"\"\"\n",
        "SELECT \n",
        "    categoria,\n",
        "    avg(ventas_totales) as promedio_ventas_diarias,\n",
        "    sum(cantidad_transacciones) as transacciones_totales\n",
        "FROM dw_analitico.ventas_resumen\n",
        "GROUP BY categoria\n",
        "ORDER BY categoria\n",
        "\"\"\"\n",
        "\n",
        "print(\"--- Ejecutando Consulta Promedio Diario ---\")\n",
        "start_time = time.time()\n",
        "\n",
        "result_promedio = client.query(query_promedio)\n",
        "\n",
        "end_time = time.time()\n",
        "tiempo_consulta_promedio = end_time - start_time\n",
        "\n",
        "print(f\"‚è±Ô∏è Tiempo de ejecuci√≥n: {tiempo_consulta_promedio:.4f} segundos\")\n",
        "print(\"\\nüìä Resultados:\")\n",
        "print(\"-\" * 60)\n",
        "for row in result_promedio.result_rows:\n",
        "    print(f\"  {row[0]}: Promedio ${row[1]:,.2f} | Transacciones: {row[2]:,}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. üìã RESUMEN FINAL DE M√âTRICAS (PARA EL INFORME)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import platform\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"üìä RESUMEN DE M√âTRICAS DE RENDIMIENTO - PROYECTO DATA PIPELINE\")\n",
        "print(\"=\"*70)\n",
        "print(f\"\\nüñ•Ô∏è Entorno de Ejecuci√≥n:\")\n",
        "print(f\"   - Plataforma: Docker Container\")\n",
        "print(f\"   - Python: {platform.python_version()}\")\n",
        "print(f\"   - Fecha: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
        "\n",
        "# Objetivos\n",
        "obj_ingesta = 300  # 5 minutos\n",
        "obj_etl = 120  # 2 minutos\n",
        "obj_query = 3  # 3 segundos\n",
        "\n",
        "print(\"\\n\" + \"-\"*70)\n",
        "print(\"\\nüìà TABLA DE TIEMPOS (Copiar al informe):\")\n",
        "print(\"\\n| Operaci√≥n                  | Tiempo Real | Objetivo    | Cumple |\")\n",
        "print(\"|:---------------------------|:-----------:|:-----------:|:------:|\")\n",
        "\n",
        "# Ingesta\n",
        "cumple_ing = \"‚úÖ\" if tiempo_ingesta < obj_ingesta else \"‚ùå\"\n",
        "print(f\"| Ingesta Cassandra (100k)   | {tiempo_ingesta:>8.2f} s  | < 5 min     | {cumple_ing}     |\")\n",
        "\n",
        "# ETL\n",
        "cumple_etl = \"‚úÖ\" if tiempo_etl < obj_etl else \"‚ùå\"\n",
        "print(f\"| Transformaci√≥n Spark ETL   | {tiempo_etl:>8.2f} s  | < 2 min     | {cumple_etl}     |\")\n",
        "\n",
        "# Consulta Top 10\n",
        "cumple_q1 = \"‚úÖ\" if tiempo_consulta_top10 < obj_query else \"‚ùå\"\n",
        "print(f\"| Consulta Top 10 (CH)       | {tiempo_consulta_top10:>8.4f} s  | < 3 seg     | {cumple_q1}     |\")\n",
        "\n",
        "# Consulta Promedio\n",
        "cumple_q2 = \"‚úÖ\" if tiempo_consulta_promedio < obj_query else \"‚ùå\"\n",
        "print(f\"| Consulta Promedio (CH)     | {tiempo_consulta_promedio:>8.4f} s  | < 3 seg     | {cumple_q2}     |\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"üí° CONCLUSI√ìN:\")\n",
        "todos_cumplen = all([\n",
        "    tiempo_ingesta < obj_ingesta, \n",
        "    tiempo_etl < obj_etl, \n",
        "    tiempo_consulta_top10 < obj_query, \n",
        "    tiempo_consulta_promedio < obj_query\n",
        "])\n",
        "if todos_cumplen:\n",
        "    print(\"   ‚úÖ TODOS los objetivos de rendimiento fueron CUMPLIDOS.\")\n",
        "else:\n",
        "    print(\"   ‚ö†Ô∏è Algunos objetivos NO fueron cumplidos. Revisar configuraci√≥n.\")\n",
        "print(\"=\"*70)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Exportar M√©tricas Finales a Markdown"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Actualizar archivo de m√©tricas con consultas\n",
        "metricas['consultas_clickhouse'] = {\n",
        "    'consulta_top10': round(tiempo_consulta_top10, 4),\n",
        "    'consulta_promedio': round(tiempo_consulta_promedio, 4),\n",
        "    'registros_analizados': total_registros,\n",
        "    'timestamp': datetime.now().isoformat()\n",
        "}\n",
        "\n",
        "with open(METRICS_FILE, 'w') as f:\n",
        "    json.dump(metricas, f, indent=2)\n",
        "\n",
        "# Generar archivo Markdown para el informe (en el mismo directorio)\n",
        "output_md = 'metricas_reales.md'\n",
        "\n",
        "with open(output_md, 'w', encoding='utf-8') as f:\n",
        "    f.write(\"# üìä M√©tricas de Rendimiento - Valores Reales\\n\\n\")\n",
        "    f.write(f\"**Fecha de ejecuci√≥n:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\\n\")\n",
        "    f.write(\"## Tabla Comparativa\\n\\n\")\n",
        "    f.write(\"| Operaci√≥n | Tiempo Real | Objetivo | Cumple |\\n\")\n",
        "    f.write(\"|:---|:---:|:---:|:---:|\\n\")\n",
        "    f.write(f\"| Ingesta Cassandra (100k) | {tiempo_ingesta:.2f} s | < 5 min | {'‚úÖ' if tiempo_ingesta < 300 else '‚ùå'} |\\n\")\n",
        "    f.write(f\"| Transformaci√≥n Spark ETL | {tiempo_etl:.2f} s | < 2 min | {'‚úÖ' if tiempo_etl < 120 else '‚ùå'} |\\n\")\n",
        "    f.write(f\"| Consulta Top 10 (ClickHouse) | {tiempo_consulta_top10:.4f} s | < 3 seg | {'‚úÖ' if tiempo_consulta_top10 < 3 else '‚ùå'} |\\n\")\n",
        "    f.write(f\"| Consulta Promedio (ClickHouse) | {tiempo_consulta_promedio:.4f} s | < 3 seg | {'‚úÖ' if tiempo_consulta_promedio < 3 else '‚ùå'} |\\n\")\n",
        "    f.write(\"\\n## Detalles de Ejecuci√≥n\\n\\n\")\n",
        "    f.write(f\"- **Registros insertados en Cassandra:** {metricas.get('ingesta_cassandra', {}).get('registros', 'N/A'):,}\\n\")\n",
        "    f.write(f\"- **Registros procesados por Spark:** {metricas.get('etl_spark', {}).get('registros_entrada', 'N/A'):,}\\n\")\n",
        "    f.write(f\"- **Registros en ClickHouse:** {total_registros:,}\\n\")\n",
        "\n",
        "print(f\"‚úÖ M√©tricas exportadas a:\")\n",
        "print(f\"   - JSON: {METRICS_FILE}\")\n",
        "print(f\"   - Markdown: {output_md}\")\n",
        "print(f\"\\nüìù Puedes copiar {output_md} a la carpeta docs/ para incluirlo en el informe.\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
