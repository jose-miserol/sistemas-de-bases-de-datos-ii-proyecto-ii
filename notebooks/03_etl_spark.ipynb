{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "134f3a7d",
   "metadata": {},
   "source": [
    "# Notebook 03: Procesamiento ETL con Spark\n",
    "\n",
    "**Universidad:** Universidad Nacional Experimental de Guayana (UNEG)  \n",
    "**Asignatura:** Sistemas de Bases de Datos II  \n",
    "**Proyecto:** Proyecto N¬∞ 2 - Data Pipeline Escalable\n",
    "\n",
    "---\n",
    "\n",
    "**Descripci√≥n:**  \n",
    "Extract (Cassandra), Transform (Spark), Load (ClickHouse)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "spark_init",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import json\n",
    "import os\n",
    "from datetime import datetime\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, sum, count\n",
    "from pyspark.sql.types import StructType, StructField, StringType, TimestampType, DecimalType, DateType, IntegerType\n",
    "\n",
    "# Archivo de m√©tricas compartido (en el directorio docs montado)\n",
    "METRICS_FILE = '../docs/metricas.json'\n",
    "\n",
    "# Crear directorio docs si no existe\n",
    "os.makedirs('../docs', exist_ok=True)\n",
    "\n",
    "# Configuraci√≥n de Spark con conectores de Cassandra y ClickHouse\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"ETL_Cassandra_Spark_ClickHouse\") \\\n",
    "    .config(\"spark.jars.packages\", \"com.datastax.spark:spark-cassandra-connector_2.12:3.5.0,com.clickhouse:clickhouse-jdbc:0.5.0\") \\\n",
    "    .config(\"spark.cassandra.connection.host\", \"cassandra\") \\\n",
    "    .config(\"spark.cassandra.connection.localDC\", \"dc1\") \\\n",
    "    .config(\"spark.driver.memory\", \"2g\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "read_cassandra",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 1. Leyendo datos de Cassandra (ventas_db.ventas_crudas) ---\n",
      "‚úÖ Lectura completada en 8.60 segundos\n",
      "Total de registros crudos: 411474\n",
      "root\n",
      " |-- fecha_venta: timestamp (nullable = false)\n",
      " |-- categoria: string (nullable = true)\n",
      " |-- id_cliente: string (nullable = true)\n",
      " |-- id_producto: string (nullable = true)\n",
      " |-- id_venta: string (nullable = true)\n",
      " |-- monto_total: decimal(38,18) (nullable = true)\n",
      "\n",
      "+-------------------+-----------+----------+-----------+--------------------+--------------------+\n",
      "|        fecha_venta|  categoria|id_cliente|id_producto|            id_venta|         monto_total|\n",
      "+-------------------+-----------+----------+-----------+--------------------+--------------------+\n",
      "|2024-07-23 14:11:00|  Alimentos|   CLI-247|   PROD-287|da6424d3-e816-44e...|1338.780000000000...|\n",
      "|2024-08-14 04:42:00|Electronica|   CLI-243|   PROD-123|785b34af-7ce3-4f8...|822.9700000000000...|\n",
      "|2024-04-16 06:00:00|      Hogar|   CLI-323|   PROD-355|4fcb7a53-1bd9-421...|970.5700000000000...|\n",
      "|2024-12-10 15:12:00|  Alimentos|    CLI-23|   PROD-962|3c6e0f39-3e02-4fc...|262.1500000000000...|\n",
      "|2024-07-28 22:15:00|  Alimentos|   CLI-329|   PROD-556|1197a9c2-76de-422...|516.8600000000000...|\n",
      "+-------------------+-----------+----------+-----------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"--- 1. Leyendo datos de Cassandra (ventas_db.ventas_crudas) ---\")\n",
    "start_read = time.time()\n",
    "\n",
    "df_raw = spark.read \\\n",
    "    .format(\"org.apache.spark.sql.cassandra\") \\\n",
    "    .options(table=\"ventas_crudas\", keyspace=\"ventas_db\") \\\n",
    "    .load()\n",
    "\n",
    "count_raw = df_raw.count()\n",
    "end_read = time.time()\n",
    "\n",
    "print(f\"‚úÖ Lectura completada en {end_read - start_read:.2f} segundos\")\n",
    "print(f\"Total de registros crudos: {count_raw}\")\n",
    "df_raw.printSchema()\n",
    "df_raw.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "transform",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 2. Transformando datos (Agregaci√≥n por Fecha y Categor√≠a) ---\n",
      "‚úÖ Transformaci√≥n completada en 5.03 segundos\n",
      "Total de filas agregadas: 1830\n"
     ]
    }
   ],
   "source": [
    "print(\"--- 2. Transformando datos (Agregaci√≥n por Fecha y Categor√≠a) ---\")\n",
    "start_transform = time.time()\n",
    "\n",
    "# Transformaci√≥n: Casting, GroupBy, Aggregation\n",
    "df_aggregated = df_raw \\\n",
    "    .withColumn(\"fecha_dia\", col(\"fecha_venta\").cast(DateType())) \\\n",
    "    .groupBy(\"fecha_dia\", \"categoria\") \\\n",
    "    .agg(\n",
    "        sum(\"monto_total\").alias(\"ventas_totales\"),\n",
    "        count(\"id_venta\").alias(\"cantidad_transacciones\")\n",
    "    )\n",
    "\n",
    "df_result = df_aggregated.select(\n",
    "    col(\"fecha_dia\").alias(\"fecha_venta\"),\n",
    "    col(\"categoria\"),\n",
    "    col(\"ventas_totales\").cast(DecimalType(18, 2)),\n",
    "    col(\"cantidad_transacciones\").cast(IntegerType())\n",
    ")\n",
    "\n",
    "# Forzamos una acci√≥n para medir el tiempo real de transformaci√≥n (Spark es lazy)\n",
    "count_result = df_result.count()\n",
    "end_transform = time.time()\n",
    "\n",
    "print(f\"‚úÖ Transformaci√≥n completada en {end_transform - start_transform:.2f} segundos\")\n",
    "print(f\"Total de filas agregadas: {count_result}\")\n",
    "df_result.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "write_clickhouse",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--- 3. Escribiendo en ClickHouse (dw_analitico.ventas_resumen) ---\")\n",
    "start_write = time.time()\n",
    "\n",
    "jdbc_url = \"jdbc:clickhouse://clickhouse:8123/dw_analitico\"\n",
    "properties = {\n",
    "    \"driver\": \"com.clickhouse.jdbc.ClickHouseDriver\"\n",
    "}\n",
    "\n",
    "try:\n",
    "    df_result.write \\\n",
    "        .mode(\"append\") \\\n",
    "        .jdbc(url=jdbc_url, table=\"ventas_resumen\", properties=properties)\n",
    "    \n",
    "    end_write = time.time()\n",
    "    print(f\"‚úÖ Carga en ClickHouse exitosa en {end_write - start_write:.2f} segundos\")\n",
    "except Exception as e:\n",
    "    end_write = time.time()\n",
    "    print(f\"‚ùå Error al escribir en ClickHouse: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "metrics_summary",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcular tiempos\n",
    "tiempo_lectura = end_read - start_read\n",
    "tiempo_transformacion = end_transform - start_transform\n",
    "tiempo_escritura = end_write - start_write\n",
    "tiempo_total_etl = tiempo_lectura + tiempo_transformacion + tiempo_escritura\n",
    "\n",
    "print(\"--- üìä Resumen de M√©tricas de Rendimiento ---\")\n",
    "print(f\"1. Lectura (Cassandra):    {tiempo_lectura:.2f} s\")\n",
    "print(f\"2. Transformaci√≥n (Spark): {tiempo_transformacion:.2f} s\")\n",
    "print(f\"3. Carga (ClickHouse):     {tiempo_escritura:.2f} s\")\n",
    "print(f\"-------------------------------------------\")\n",
    "print(f\"Tiempo Total ETL:          {tiempo_total_etl:.2f} s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "save_metrics",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================================\n",
    "# üìä GUARDAR M√âTRICAS PARA NOTEBOOK 04\n",
    "# =====================================================\n",
    "\n",
    "# Leer m√©tricas existentes o crear nuevo diccionario\n",
    "if os.path.exists(METRICS_FILE):\n",
    "    with open(METRICS_FILE, 'r') as f:\n",
    "        metricas = json.load(f)\n",
    "else:\n",
    "    metricas = {}\n",
    "\n",
    "# Actualizar con m√©tricas de este notebook\n",
    "metricas['etl_spark'] = {\n",
    "    'tiempo_lectura': round(tiempo_lectura, 2),\n",
    "    'tiempo_transformacion': round(tiempo_transformacion, 2),\n",
    "    'tiempo_escritura': round(tiempo_escritura, 2),\n",
    "    'tiempo_total': round(tiempo_total_etl, 2),\n",
    "    'registros_entrada': count_raw,\n",
    "    'registros_salida': count_result,\n",
    "    'timestamp': datetime.now().isoformat()\n",
    "}\n",
    "\n",
    "# Guardar en docs\n",
    "with open(METRICS_FILE, 'w') as f:\n",
    "    json.dump(metricas, f, indent=2)\n",
    "\n",
    "print(f\"‚úÖ M√©tricas guardadas en: {METRICS_FILE}\")\n",
    "print(f\"   - Tiempo total ETL: {tiempo_total_etl:.2f} segundos\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
