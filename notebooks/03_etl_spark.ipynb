{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "92014a2e-1fa5-4df7-80d3-17da6247b090",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notebook 03: Procesamiento ETL con Spark\n",
    "# **Universidad:** Universidad Nacional Experimental de Guayana (UNEG)  \n",
    "# **Asignatura:** Sistemas de Bases de Datos II  \n",
    "# **Proyecto:** Proyecto N2 - Data Pipeline Escalable\n",
    "# ---\n",
    "# **Descripci√≥n:**  \n",
    "# Extract (Cassandra), Transform (Spark), Load (ClickHouse)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "spark_init",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import json\n",
    "import os\n",
    "from datetime import datetime\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, sum, count\n",
    "from pyspark.sql.types import StructType, StructField, StringType, TimestampType, DecimalType, DateType, IntegerType\n",
    "\n",
    "# Archivo de m√©tricas compartido (directorio docs montado por Docker)\n",
    "METRICS_FILE = '../docs/metricas.json'\n",
    "\n",
    "# Configuraci√≥n de Spark con conectores de Cassandra y ClickHouse\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"ETL_Cassandra_Spark_ClickHouse\") \\\n",
    "    .config(\"spark.jars.packages\", \"com.datastax.spark:spark-cassandra-connector_2.12:3.5.0,com.clickhouse:clickhouse-jdbc:0.5.0\") \\\n",
    "    .config(\"spark.cassandra.connection.host\", \"cassandra\") \\\n",
    "    .config(\"spark.cassandra.connection.localDC\", \"dc1\") \\\n",
    "    .config(\"spark.driver.memory\", \"2g\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "read_cassandra",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 1. Leyendo datos de Cassandra (ventas_db.ventas_crudas) ---\n",
      "‚úÖ Lectura completada en 5.85 segundos\n",
      "Total de registros crudos: 100000\n",
      "root\n",
      " |-- fecha_venta: date (nullable = false)\n",
      " |-- id_venta: string (nullable = true)\n",
      " |-- cantidad: integer (nullable = true)\n",
      " |-- categoria: string (nullable = true)\n",
      " |-- id_cliente: string (nullable = true)\n",
      " |-- id_producto: string (nullable = true)\n",
      " |-- metodo_pago: string (nullable = true)\n",
      " |-- monto_total: decimal(38,18) (nullable = true)\n",
      " |-- precio_unitario: decimal(38,18) (nullable = true)\n",
      " |-- producto: string (nullable = true)\n",
      " |-- region: string (nullable = true)\n",
      "\n",
      "+-----------+--------------------+--------+----------+--------------------+--------------------+---------------+--------------------+--------------------+-----------+------+\n",
      "|fecha_venta|            id_venta|cantidad| categoria|          id_cliente|         id_producto|    metodo_pago|         monto_total|     precio_unitario|   producto|region|\n",
      "+-----------+--------------------+--------+----------+--------------------+--------------------+---------------+--------------------+--------------------+-----------+------+\n",
      "| 2024-07-29|03237c10-799c-47a...|       7|    Libros|7f525b56-4a5e-4f3...|c974f9ea-748e-4ef...|       Efectivo|425.9500000000000...|60.85000000000000...|Producto 29| Norte|\n",
      "| 2024-07-29|032682bb-d37b-496...|       6|    Libros|151d420e-9f49-453...|49ef40a3-c827-4ff...| Tarjeta Debito|428.8800000000000...|71.48000000000000...|Producto 98| Oeste|\n",
      "| 2024-07-29|0335f39a-5da7-454...|      10|Automotriz|3e529f51-ae5d-4bc...|2f76a34f-7411-470...|  Transferencia|1317.500000000000...|131.7500000000000...|Producto 61|Centro|\n",
      "| 2024-07-29|043489d3-dc75-410...|       3|    Libros|9c1ea9e9-c95e-415...|e3d719f7-2bb1-4e0...| Tarjeta Debito|376.2600000000000...|125.4200000000000...|Producto 91| Norte|\n",
      "| 2024-07-29|04e7c704-4eb4-435...|       8|     Salud|5184fc66-d3a3-49d...|1da74b52-a9fd-4cb...|Tarjeta Credito|382.0000000000000...|47.75000000000000...|Producto 78|  Este|\n",
      "+-----------+--------------------+--------+----------+--------------------+--------------------+---------------+--------------------+--------------------+-----------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"--- 1. Leyendo datos de Cassandra (ventas_db.ventas_crudas) ---\")\n",
    "start_read = time.time()\n",
    "\n",
    "df_raw = spark.read \\\n",
    "    .format(\"org.apache.spark.sql.cassandra\") \\\n",
    "    .options(table=\"ventas_crudas\", keyspace=\"ventas_db\") \\\n",
    "    .load()\n",
    "\n",
    "count_raw = df_raw.count()\n",
    "end_read = time.time()\n",
    "\n",
    "print(f\"‚úÖ Lectura completada en {end_read - start_read:.2f} segundos\")\n",
    "print(f\"Total de registros crudos: {count_raw}\")\n",
    "df_raw.printSchema()\n",
    "df_raw.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "transform",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 2. Transformando datos (Agregaci√≥n por Fecha y Categor√≠a) ---\n",
      "‚úÖ Transformaci√≥n completada en 3.17 segundos\n",
      "Total de filas agregadas after cleaning: 5490\n",
      "+-----------+----------+--------------+----------------------+\n",
      "|fecha_venta| categoria|ventas_totales|cantidad_transacciones|\n",
      "+-----------+----------+--------------+----------------------+\n",
      "| 2024-07-21| Alimentos|       6768.01|                    18|\n",
      "| 2024-07-21|    Libros|       5280.44|                    14|\n",
      "| 2024-02-06|     Hogar|       4114.64|                    16|\n",
      "| 2024-10-31|   Belleza|       3851.69|                    19|\n",
      "| 2024-10-31|Automotriz|       7153.07|                    16|\n",
      "+-----------+----------+--------------+----------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"--- 2. Transformando datos (Agregaci√≥n por Fecha y Categor√≠a) ---\")\n",
    "start_transform = time.time()\n",
    "\n",
    "# Limpieza de datos (Fase 3.3): Filtrar montos nulos o negativos\n",
    "df_cleaned = df_raw.filter((col(\"monto_total\").isNotNull()) & (col(\"monto_total\") > 0))\n",
    "\n",
    "# Transformaci√≥n: Casting, GroupBy, Aggregation\n",
    "df_aggregated = df_cleaned \\\n",
    "    .withColumn(\"fecha_dia\", col(\"fecha_venta\").cast(DateType())) \\\n",
    "    .groupBy(\"fecha_dia\", \"categoria\") \\\n",
    "    .agg(\n",
    "        sum(\"monto_total\").alias(\"ventas_totales\"),\n",
    "        count(\"id_venta\").alias(\"cantidad_transacciones\")\n",
    "    )\n",
    "\n",
    "df_result = df_aggregated.select(\n",
    "    col(\"fecha_dia\").alias(\"fecha_venta\"),\n",
    "    col(\"categoria\"),\n",
    "    col(\"ventas_totales\").cast(DecimalType(18, 2)),\n",
    "    col(\"cantidad_transacciones\").cast(IntegerType())\n",
    ")\n",
    "\n",
    "# Forzamos una acci√≥n para medir el tiempo real de transformaci√≥n (Spark es lazy)\n",
    "count_result = df_result.count()\n",
    "end_transform = time.time()\n",
    "\n",
    "print(f\"‚úÖ Transformaci√≥n completada en {end_transform - start_transform:.2f} segundos\")\n",
    "print(f\"Total de filas agregadas after cleaning: {count_result}\")\n",
    "df_result.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "write_clickhouse",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 3. Escribiendo en ClickHouse (dw_analitico.ventas_resumen) ---\n",
      "‚úÖ Carga en ClickHouse exitosa en 1.95 segundos\n"
     ]
    }
   ],
   "source": [
    "print(\"--- 3. Escribiendo en ClickHouse (dw_analitico.ventas_resumen) ---\")\n",
    "start_write = time.time()\n",
    "\n",
    "jdbc_url = \"jdbc:clickhouse://clickhouse:8123/dw_analitico\"\n",
    "properties = {\n",
    "    \"driver\": \"com.clickhouse.jdbc.ClickHouseDriver\"\n",
    "}\n",
    "\n",
    "try:\n",
    "    df_result.write \\\n",
    "        .mode(\"append\") \\\n",
    "        .jdbc(url=jdbc_url, table=\"ventas_resumen\", properties=properties)\n",
    "    \n",
    "    end_write = time.time()\n",
    "    print(f\"‚úÖ Carga en ClickHouse exitosa en {end_write - start_write:.2f} segundos\")\n",
    "except Exception as e:\n",
    "    end_write = time.time()\n",
    "    print(f\"‚ùå Error al escribir en ClickHouse: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "metrics_summary",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- üìä Resumen de M√©tricas de Rendimiento ---\n",
      "1. Lectura (Cassandra):    5.85 s\n",
      "2. Transformaci√≥n (Spark): 3.17 s\n",
      "3. Carga (ClickHouse):     1.95 s\n",
      "-------------------------------------------\n",
      "Tiempo Total ETL:          10.97 s\n"
     ]
    }
   ],
   "source": [
    "# Calcular tiempos\n",
    "tiempo_lectura = end_read - start_read\n",
    "tiempo_transformacion = end_transform - start_transform\n",
    "tiempo_escritura = end_write - start_write\n",
    "tiempo_total_etl = tiempo_lectura + tiempo_transformacion + tiempo_escritura\n",
    "\n",
    "print(\"--- üìä Resumen de M√©tricas de Rendimiento ---\")\n",
    "print(f\"1. Lectura (Cassandra):    {tiempo_lectura:.2f} s\")\n",
    "print(f\"2. Transformaci√≥n (Spark): {tiempo_transformacion:.2f} s\")\n",
    "print(f\"3. Carga (ClickHouse):     {tiempo_escritura:.2f} s\")\n",
    "print(f\"-------------------------------------------\")\n",
    "print(f\"Tiempo Total ETL:          {tiempo_total_etl:.2f} s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "save_metrics",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ M√©tricas guardadas en: ../docs/metricas.json\n",
      "   - Tiempo total ETL: 10.97 segundos\n"
     ]
    }
   ],
   "source": [
    "# =====================================================\n",
    "# üìä GUARDAR M√âTRICAS PARA NOTEBOOK 04\n",
    "# =====================================================\n",
    "\n",
    "# Leer m√©tricas existentes o crear nuevo diccionario\n",
    "if os.path.exists(METRICS_FILE):\n",
    "    with open(METRICS_FILE, 'r') as f:\n",
    "        metricas = json.load(f)\n",
    "else:\n",
    "    metricas = {}\n",
    "\n",
    "# Actualizar con m√©tricas de este notebook\n",
    "metricas['etl_spark'] = {\n",
    "    'tiempo_lectura': round(tiempo_lectura, 2),\n",
    "    'tiempo_transformacion': round(tiempo_transformacion, 2),\n",
    "    'tiempo_escritura': round(tiempo_escritura, 2),\n",
    "    'tiempo_total': round(tiempo_total_etl, 2),\n",
    "    'registros_entrada': count_raw,\n",
    "    'registros_salida': count_result,\n",
    "    'timestamp': datetime.now().isoformat()\n",
    "}\n",
    "\n",
    "# Guardar en docs\n",
    "with open(METRICS_FILE, 'w') as f:\n",
    "    json.dump(metricas, f, indent=2)\n",
    "\n",
    "print(f\"‚úÖ M√©tricas guardadas en: {METRICS_FILE}\")\n",
    "print(f\"   - Tiempo total ETL: {tiempo_total_etl:.2f} segundos\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
