{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ‚ö° Notebook 03: Procesamiento ETL con Spark\n",
        "\n",
        "**Universidad:** Universidad Nacional Experimental de Guayana (UNEG)  \n",
        "**Asignatura:** Sistemas de Bases de Datos II  \n",
        "**Proyecto:** Proyecto N¬∞ 2 - Data Pipeline Escalable\n",
        "\n",
        "---\n",
        "\n",
        "**Descripci√≥n:**  \n",
        "Extract (Cassandra), Transform (Spark), Load (ClickHouse)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "spark_init",
      "metadata": {},
      "outputs": [],
      "source": [
        "import time\n",
        "import json\n",
        "import os\n",
        "from datetime import datetime\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, sum, count\n",
        "from pyspark.sql.types import StructType, StructField, StringType, TimestampType, DecimalType, DateType, IntegerType\n",
        "\n",
        "# Archivo de m√©tricas compartido (en el mismo directorio de notebooks)\n",
        "METRICS_FILE = 'metricas.json'\n",
        "\n",
        "# Configuraci√≥n de Spark con conectores de Cassandra y ClickHouse\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"ETL_Cassandra_Spark_ClickHouse\") \\\n",
        "    .config(\"spark.jars.packages\", \"com.datastax.spark:spark-cassandra-connector_2.12:3.5.0,com.clickhouse:clickhouse-jdbc:0.5.0\") \\\n",
        "    .config(\"spark.cassandra.connection.host\", \"cassandra\") \\\n",
        "    .config(\"spark.cassandra.connection.localDC\", \"dc1\") \\\n",
        "    .config(\"spark.driver.memory\", \"2g\") \\\n",
        "    .getOrCreate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "read_cassandra",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- 1. Leyendo datos de Cassandra (ventas_db.ventas_crudas) ---\n",
            "‚úÖ Lectura completada en 1.17 segundos\n",
            "Total de registros crudos: 358183\n",
            "root\n",
            " |-- fecha_venta: timestamp (nullable = false)\n",
            " |-- categoria: string (nullable = true)\n",
            " |-- id_cliente: string (nullable = true)\n",
            " |-- id_producto: string (nullable = true)\n",
            " |-- id_venta: string (nullable = true)\n",
            " |-- monto_total: decimal(38,18) (nullable = true)\n",
            "\n",
            "+-------------------+-----------+----------+-----------+--------------------+--------------------+\n",
            "|        fecha_venta|  categoria|id_cliente|id_producto|            id_venta|         monto_total|\n",
            "+-------------------+-----------+----------+-----------+--------------------+--------------------+\n",
            "|2024-03-08 14:06:00|      Hogar|   CLI-389|   PROD-511|9baf20d0-bb22-487...|1323.910000000000...|\n",
            "|2024-11-10 23:25:00|Electronica|   CLI-219|   PROD-784|736106c8-7eff-45b...|742.3100000000000...|\n",
            "|2024-08-21 23:07:00|       Ropa|   CLI-393|   PROD-329|6d585839-76ef-4e8...|1422.000000000000...|\n",
            "|2024-05-21 08:07:00|      Hogar|   CLI-139|   PROD-955|205deb3e-5b68-4c7...|281.9300000000000...|\n",
            "|2024-03-05 18:13:00|      Hogar|    CLI-47|   PROD-649|d6a5bbde-4984-488...|819.8500000000000...|\n",
            "|2024-05-21 08:07:00|      Hogar|   CLI-139|   PROD-955|205deb3e-5b68-4c7...|281.9300000000000...|\n",
            "+-------------------+-----------+----------+-----------+--------------------+--------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(\"--- 1. Leyendo datos de Cassandra (ventas_db.ventas_crudas) ---\")\n",
        "start_read = time.time()\n",
        "\n",
        "df_raw = spark.read \\\n",
        "    .format(\"org.apache.spark.sql.cassandra\") \\\n",
        "    .options(table=\"ventas_crudas\", keyspace=\"ventas_db\") \\\n",
        "    .load()\n",
        "\n",
        "count_raw = df_raw.count()\n",
        "end_read = time.time()\n",
        "\n",
        "print(f\"‚úÖ Lectura completada en {end_read - start_read:.2f} segundos\")\n",
        "print(f\"Total de registros crudos: {count_raw}\")\n",
        "df_raw.printSchema()\n",
        "df_raw.show(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "transform",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- 2. Transformando datos (Agregaci√≥n por Fecha y Categor√≠a) ---\n",
            "‚úÖ Transformaci√≥n completada en 2.30 segundos\n",
            "Total de filas agregadas: 1830\n",
            "+-----------+-----------+--------------+----------------------+\n",
            "|fecha_venta|  categoria|ventas_totales|cantidad_transacciones|\n",
            "+-----------+-----------+--------------+----------------------+\n",
            "| 2024-08-14|   Deportes|     145253.12|                   197|\n",
            "| 2024-07-02|      Hogar|     132301.35|                   176|\n",
            "| 2024-02-20|       Ropa|     169464.82|                   211|\n",
            "| 2024-10-30|   Deportes|     158106.33|                   203|\n",
            "| 2024-03-26|Electronica|     153805.73|                   197|\n",
            "+-----------+-----------+--------------+----------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(\"--- 2. Transformando datos (Agregaci√≥n por Fecha y Categor√≠a) ---\")\n",
        "start_transform = time.time()\n",
        "\n",
        "# Transformaci√≥n: Casting, GroupBy, Aggregation\n",
        "df_aggregated = df_raw \\\n",
        "    .withColumn(\"fecha_dia\", col(\"fecha_venta\").cast(DateType())) \\\n",
        "    .groupBy(\"fecha_dia\", \"categoria\") \\\n",
        "    .agg(\n",
        "        sum(\"monto_total\").alias(\"ventas_totales\"),\n",
        "        count(\"id_venta\").alias(\"cantidad_transacciones\")\n",
        "    )\n",
        "\n",
        "df_result = df_aggregated.select(\n",
        "    col(\"fecha_dia\").alias(\"fecha_venta\"),\n",
        "    col(\"categoria\"),\n",
        "    col(\"ventas_totales\").cast(DecimalType(18, 2)),\n",
        "    col(\"cantidad_transacciones\").cast(IntegerType())\n",
        ")\n",
        "\n",
        "# Forzamos una acci√≥n para medir el tiempo real de transformaci√≥n (Spark es lazy)\n",
        "count_result = df_result.count()\n",
        "end_transform = time.time()\n",
        "\n",
        "print(f\"‚úÖ Transformaci√≥n completada en {end_transform - start_transform:.2f} segundos\")\n",
        "print(f\"Total de filas agregadas: {count_result}\")\n",
        "df_result.show(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "write_clickhouse",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- 3. Escribiendo en ClickHouse (dw_analitico.ventas_resumen) ---\n",
            "‚úÖ Carga en ClickHouse exitosa en 2.22 segundos\n"
          ]
        }
      ],
      "source": [
        "print(\"--- 3. Escribiendo en ClickHouse (dw_analitico.ventas_resumen) ---\")\n",
        "start_write = time.time()\n",
        "\n",
        "jdbc_url = \"jdbc:clickhouse://clickhouse:8123/dw_analitico\"\n",
        "properties = {\n",
        "    \"driver\": \"com.clickhouse.jdbc.ClickHouseDriver\"\n",
        "}\n",
        "\n",
        "try:\n",
        "    df_result.write \\\n",
        "        .mode(\"append\") \\\n",
        "        .jdbc(url=jdbc_url, table=\"ventas_resumen\", properties=properties)\n",
        "    \n",
        "    end_write = time.time()\n",
        "    print(f\"‚úÖ Carga en ClickHouse exitosa en {end_write - start_write:.2f} segundos\")\n",
        "except Exception as e:\n",
        "    end_write = time.time()\n",
        "    print(f\"‚ùå Error al escribir en ClickHouse: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "metrics_summary",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- üìä Resumen de M√©tricas de Rendimiento ---\n",
            "1. Lectura (Cassandra):    1.17 s\n",
            "2. Transformaci√≥n (Spark): 2.30 s\n",
            "3. Carga (ClickHouse):     2.22 s\n",
            "-------------------------------------------\n",
            "Tiempo Total ETL:          5.70 s\n"
          ]
        }
      ],
      "source": [
        "# Calcular tiempos\n",
        "tiempo_lectura = end_read - start_read\n",
        "tiempo_transformacion = end_transform - start_transform\n",
        "tiempo_escritura = end_write - start_write\n",
        "tiempo_total_etl = tiempo_lectura + tiempo_transformacion + tiempo_escritura\n",
        "\n",
        "print(\"--- üìä Resumen de M√©tricas de Rendimiento ---\")\n",
        "print(f\"1. Lectura (Cassandra):    {tiempo_lectura:.2f} s\")\n",
        "print(f\"2. Transformaci√≥n (Spark): {tiempo_transformacion:.2f} s\")\n",
        "print(f\"3. Carga (ClickHouse):     {tiempo_escritura:.2f} s\")\n",
        "print(f\"-------------------------------------------\")\n",
        "print(f\"Tiempo Total ETL:          {tiempo_total_etl:.2f} s\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "save_metrics",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ M√©tricas guardadas en: metricas.json\n",
            "   - Tiempo total ETL: 5.70 segundos\n"
          ]
        }
      ],
      "source": [
        "# =====================================================\n",
        "# üìä GUARDAR M√âTRICAS PARA NOTEBOOK 04\n",
        "# =====================================================\n",
        "\n",
        "# Leer m√©tricas existentes o crear nuevo diccionario\n",
        "if os.path.exists(METRICS_FILE):\n",
        "    with open(METRICS_FILE, 'r') as f:\n",
        "        metricas = json.load(f)\n",
        "else:\n",
        "    metricas = {}\n",
        "\n",
        "# Actualizar con m√©tricas de este notebook\n",
        "metricas['etl_spark'] = {\n",
        "    'tiempo_lectura': round(tiempo_lectura, 2),\n",
        "    'tiempo_transformacion': round(tiempo_transformacion, 2),\n",
        "    'tiempo_escritura': round(tiempo_escritura, 2),\n",
        "    'tiempo_total': round(tiempo_total_etl, 2),\n",
        "    'registros_entrada': count_raw,\n",
        "    'registros_salida': count_result,\n",
        "    'timestamp': datetime.now().isoformat()\n",
        "}\n",
        "\n",
        "# Guardar en el mismo directorio de notebooks\n",
        "with open(METRICS_FILE, 'w') as f:\n",
        "    json.dump(metricas, f, indent=2)\n",
        "\n",
        "print(f\"‚úÖ M√©tricas guardadas en: {METRICS_FILE}\")\n",
        "print(f\"   - Tiempo total ETL: {tiempo_total_etl:.2f} segundos\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
